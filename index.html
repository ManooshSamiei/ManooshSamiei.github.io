<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Manoosh Samiei</title>
  
  <meta name="author" content="Manoosh Samiei">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Manoosh Samiei</name>
              </p>
              <p style="text-align:justify">I am a PhD student in NeuroAI at <a href="https://www.cs.mcgill.ca/research/">McGill Computer Science</a> and <a href="https://mila.quebec/en/">Mila</a>, supervised by Professors Doina Precup and Paul Masset. I study the interplay between short- and long-term decision-making in the human brain using reinforcement learning and dopaminergic neuron modeling. Previously I worked for two years as a computer vision researcher and 3D reconstruction engineer at <a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjz5Y2R4bj5AhUbkYkEHfw0Cn0QFnoECBMQAQ&url=https%3A%2F%2Falgolux.com%2F&usg=AOvVaw3FVHvRDBmZHK__oN5p6Mgh">Algolux</a>, a self-driving car software startup, and <a href="https://www.magicplan.app/">Magicplan, Sensopia Inc.</a>, an augmented reality mobile app which maps indoor environments.
</p>
<p style="text-align:justify">
I did my masters at <a href="https://www.mcgill.ca/ece">McGill</a> in Electrical and Computer Engineering, where I was advised by Prof. James J. Clark. My master's research was on modeling visual attention and distraction, which is closely related to the saliency prediction, in visual search tasks using deep learning and eye tracking data.
                 </p>
              <p style="text-align:justify">            
I completed my undergraduate in Electrical Engineering at Shahid Beheshti University, focusing on telecommunications and signal processing. My undergrad research involved end-to-end lane following in autonomous vehicles using CNNs. </p>
              <p style="text-align:justify">  My research interests include cognitive science, AI, and applications in healthcare. Besides AI and programming, I also enjoy camping and spending unlimited time with cats and in nature!
              </p>
              <p style="text-align:center">
                <a href="mailto:manoosh.samiei@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/manoosh-samiei-2386a1190/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=PEVAoSkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/SamieiManoosh">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ManooshSamiei">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Manoosh_.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/manoosh.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Highlights</heading>
              <p>
                <strong>15/09/2023</strong> I am joining Mila soon as an AI researcher!
              </p>
              <p>
                <strong>09/08/2022</strong> Moving to Toronto, Ontario on 1st September!
              </p>
              <p>
                <strong>01/08/2021</strong> I am officially graduated from McGill with a Master of Science in Electrical Engineering!
              </p>
              <p>
                <strong>16/08/2021</strong> I am starting as a computer vision researcher at Algolux! 
              </p>
            </td>
          </tr>
        </tbody></table>
           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" id='visual_search'>
                  <img src='images/visual-search.jpg' width="160"></div>
              <script type="text/javascript">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              	Master Research, 2021<br>
                <papertitle>Predicting Visual Attention and Distraction During Visual Search Using Convolutional Neural Networks</papertitle>
              <br>
              Manoosh Samiei, James J. Clark
              <br>
              <a href="https://github.com/ManooshSamiei/Distraction-Visual-Search">GitHub Code</a>
              /
              <a href="https://escholarship.mcgill.ca/concern/theses/5t34sq19q">Thesis</a>
              <br>
              Our dataset analysis report is available on <em>Arxiv</em>: 
              <a href="https://github.com/ManooshSamiei/COCOSearch18_Analysis">Code</a>
              /
              <a href="https://arxiv.org/abs/2209.13771">Paper</a> 
              <br>          
               One publication in <em>Journal of Vision</em> is in progress. <a href="https://arxiv.org/abs/2210.15093#">ArXiv Pre-print</a>   
              <p></p>
              <p>
We present two approaches. Our first method uses a two-stream encoder-decoder network to predict fixation density maps of human observers in visual search. Our second method predicts the segmentation of distractor and target objects during search using a Mask-RCNN segmentation network. We use COCO-Search18 dataset to train/finetune and evaluate our models.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='deepgaze'>
                <img src='images/deepgaze.jpg' width="160">
              </div>
              <script type="text/javascript">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
                <papertitle>Implementing DeepGaze2 Free-viewing Saliency Model, 2020</papertitle>
              <br>
              <a href="https://github.com/ManooshSamiei/DeepGaze2_SaliencyModel">GitHub</a>
              /
              <a href="https://github.com/ManooshSamiei/DeepGaze2_SaliencyModel/blob/main/Report.pdf">Report</a>
              <p></p>
              <p>DeepGaze2 extracts high-level features in images using VGG19 convolutional neural network pretrained for object recognition task. DeepGaze II is trained using a log-likelihood learning framework, and aims to predict where humans look while free-viewing a set of images.</p>
            </td>
          </tr> 
          				
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='OD_RL'>

                <img src='images/OD_RL_1.png' width="160">
              </div>
              <script type="text/javascript">

              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
                <papertitle>Object Detection with Deep Reinforcement Learning, 2020</papertitle>
              <br>
							<a href="https://github.com/ManooshSamiei/Object-Detection-Deep-Reinforcement-Learning">GitHub</a> / 
							<a href="https://arxiv.org/abs/2208.04511">Report</a> / 
							<a href="https://youtu.be/dcGP9mDnFf0">video</a>			
              <p></p>
              <p>We implmented two papers that formulate object localization as a dynamic Markov decision process based on deep reinforcement learning. We compare two different action settings for this MDP: a hierarchical method and a dynamic method. </p>
            </td>
          </tr>

			    <tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one" id='3d_object'>
			          <img src='images/3D.png' width="160">
			        </div>
			        <script type="text/javascript">
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:top">
			            NeurIPS 2019 Reproducibility Challenge
			            <br>
			            <papertitle>Reproducing CNN2: Viewpoint Generalization via a Binocular Vision, 2019</papertitle>
			          <br>
			          <a href="https://openreview.net/forum?id=jUJo2RYTOb">Report</a>
			          <p></p>
			          <p> We replicated the results of the paper ‚ÄúCNN2: Viewpoint Generalization via a Binocular Vision‚Äù for two datasets SmallNORB and ModelNet2D.</p>
			        </td>
			      </tr>
		
			    <tr>
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one" id='slf'>
			          <img src='images/slf.jpg' width="160">
			        </div>
			        <script type="text/javascript">
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:top">
			            <papertitle>Implementation of End-to-End Behavioral Cloning Approach for Lane Following Task in Autonomous Vehicles using Convolutional Neural Networks, 2019</papertitle>
			          <br>
			          <a href="https://drive.google.com/file/d/11b24VbK3d9jYsrMFm8ikz8AWEWoQKFgA/view?usp=sharing">Thesis in Persian</a>
			          <p> </p>
			        </td>
			      </tr>				
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/poster.png" width="140"></td>
            <td width="75%" valign="top">
              <br>
              Volunteer in poster sessions in <a href="http://montrealaisymposium.com/"> Montreal AI Symposium 2020</a>
              , and 
              <a href="https://wimlworkshop.org/neurips2020/">WiML 2020</a>
              <br>
              Helped with locating posters and technical issues in gather town platform.
            </td>
          </tr>


					
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Source code and style from <a href="https://github.com/jonbarron/jonbarron_website"> Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
